{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "187e6115",
   "metadata": {},
   "source": [
    "# Memory Estimator \n",
    "\n",
    "This notebook will provide some examples on how to use the memory_estimator API\n",
    "to estimate the amount of GPU memory consumed when fine-tuning an LLM model in Training Hub.\n",
    "This notebook will cover:\n",
    "1. How the package's primary class implemented, \n",
    "2. How it can be subclassed for further extensions,\n",
    "3. How it can be used via both class instantiation and via convenience function,\n",
    "\n",
    "Tips on how LLM memory usage is calculated and how the memory can be reduced will also be mentioned as needed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a08d4d7c",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8274236",
   "metadata": {},
   "outputs": [],
   "source": [
    "from training_hub.profiling.memory_estimator import BasicEstimator, OSFTEstimator, estimate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c61f401a",
   "metadata": {},
   "source": [
    "The estimation depends on several key factors that should be user inputted. These are:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e3515f5",
   "metadata": {},
   "source": [
    "#### The Pre-Trained Model to be Fine-Tuned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "66208c2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"ibm-granite/granite-3.3-2b-instruct\"  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b98e920e",
   "metadata": {},
   "source": [
    "#### The Number and Size of Your GPUs\n",
    "\n",
    "The given default values will assume you are training on 2x L40s, each containing 48 GB of memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70462895",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_gpus = 2\n",
    "gpu_memory = 48 * (2**30) # 48 GB in bytes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cf719d0",
   "metadata": {},
   "source": [
    "#### The Maximum Number of Tokens You'll Place Onto a GPU\n",
    "\n",
    "Note that in training hub, minibatches will be operated in such a way that\n",
    "the number of tokens on the GPU never exceeds this value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a735ecbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_tokens_per_gpu = 8192"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62eba14c",
   "metadata": {},
   "source": [
    "## Profiler Overview\n",
    "\n",
    "At a lower level, the profiling module provides a class `BasicEstimator` that implements the memory estimation for training an LLM normally (via SFT).\n",
    "\n",
    "The estimator computes this values in the `estimate` function through the following procedure:\n",
    "\n",
    "1. Calculate the memory needed to store the model parameters (`_calc_model_params`)\n",
    "\n",
    "2. Calculate the memory needed to store the model's gradients (`_calc_gradients`)\n",
    "\n",
    "3. Calculate the memory needed to store the model's optimizer states (`_calc_optimizer`)\n",
    "    - The values of Steps 1-3 is proportional to the number of parameters within the the model.\n",
    "    - This estimator assumes the AdamW optimizer, which stores 2 optimizer parameters per model parameter\n",
    "        - Some non-Adam optimizers use only 1 optimizer parameter, although training hub uses AdamW by default\n",
    "\n",
    "4. Calculate the memory needed to store the intermediate activations within the model (`_calc_intermediate_activations`)\n",
    "    - This value is the product of the number of tokens being passed onto a GPU, the number of layers in the model, and the model's hidden dimensionality\n",
    "\n",
    "5. Calculate the memory needed to store the activated output the model (`_calc_outputs`)\n",
    "    - This value is the product of the number of tokens being passed onto a GPU and the vocabulary size of the model.\n",
    "\n",
    "6. Calculate any additional memory the model might use (this value is 0 for SFT) (`_calc_additional`)\n",
    "\n",
    "7. Sum up the memory calculated in Steps 1-6\n",
    "\n",
    "8. Apply multiplers representing possible overhead to get the low bound (1x), expected (1.1x), and upper bound (1.3x) for the memory usage of this model (`_apply_overhead`)\n",
    "\n",
    "Note that training hub assumes that all of the above values are stored in Float32 (4 bytes per tensor entry)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e2277c9",
   "metadata": {},
   "source": [
    "## Basic SFT Estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e0486d2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  2.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimations for ibm-granite/granite-3.3-2b-instruct:\n",
      "\n",
      "\n",
      "Summary:\n",
      "The expected amount of memory needed to run this model is about 55.8 GB\n",
      "The lower and upper bounds are 50.8 - 66.0 GB\n",
      "If you have 2 GPUs, you will need about 27.9 GB, with bounds of 25.4 - 33.0 GB per GPU\n",
      "\n",
      "\n",
      "Component Breakdown:\n",
      "Each GPU will need 4.7 GB to store the model parameters\n",
      "Each GPU will need 9.4 GB to store the optimizer states\n",
      "Each GPU will need 4.7 GB to store the gradients\n",
      "Each GPU will need 2.5 GB to store the intermediate activations\n",
      "Each GPU will need 4.0 GB to store the outputs\n",
      "Up to 7.6 GB can be expected as overhead\n",
      "\n",
      "Decision:\n",
      "The proposed training setup should work for your hardware.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "my_sft_estimator = BasicEstimator(num_gpus=num_gpus,\n",
    "                                    gpu_memory=gpu_memory,\n",
    "                                    model_path=model_path,\n",
    "                                    max_tokens_per_gpu=max_tokens_per_gpu,\n",
    "                                    verbose=2\n",
    "                                )\n",
    "\n",
    "sft_lower_bound, sft_expected, sft_upper_bound = my_sft_estimator.estimate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4396155f",
   "metadata": {},
   "source": [
    "## OSFT Estimation and Subclassing\n",
    "Training Hub plans to implement a wide variety of different methods for training LLMs,\n",
    "with OSFT having been recently implemented.\n",
    "\n",
    "Because the estimator is implemented as a class, the individual components for\n",
    "calculating the memory are their own functions, and LLM methods tend to have similarities\n",
    "in how they consume memories, we can create new estimators by simply subclassing `BasicEstimator`\n",
    "and overriding any of the respective methods for the individual pieces of memory computation\n",
    "with formulas that are more accurate for that training method.\n",
    "\n",
    "For example, the estimator for OSFT is implemented as the subclass `OSFTEstimator`.\n",
    "On top of some under-the-hood changes, its main adjustment is overriding `_calc_model_params`\n",
    "to use the U, Sigma, and V matrices obtained through SVD calculation instead of the typical\n",
    "model weight matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "93b6afc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  2.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimations for ibm-granite/granite-3.3-2b-instruct:\n",
      "\n",
      "\n",
      "Summary:\n",
      "The expected amount of memory needed to run this model is about 84.0 GB\n",
      "The lower and upper bounds are 76.4 - 99.3 GB\n",
      "If you have 2 GPUs, you will need about 42.0 GB, with bounds of 38.2 - 49.7 GB per GPU\n",
      "\n",
      "\n",
      "Component Breakdown:\n",
      "Each GPU will need 18.0 GB to store the model parameters\n",
      "Each GPU will need 9.4 GB to store the optimizer states\n",
      "Each GPU will need 4.7 GB to store the gradients\n",
      "Each GPU will need 2.5 GB to store the intermediate activations\n",
      "Each GPU will need 3.5 GB to store the outputs\n",
      "Up to 11.5 GB can be expected as overhead\n",
      "\n",
      "Decision:\n",
      "The proposed training setup will likely work for your hardware.\n",
      "\n",
      "Ideal amount of extra memory required (to reach the\n",
      "upper bound of memory requirements): 1.7 GB\n"
     ]
    }
   ],
   "source": [
    "my_osft_estimator = OSFTEstimator(num_gpus=num_gpus,\n",
    "                                    gpu_memory=gpu_memory,\n",
    "                                    model_path=model_path,\n",
    "                                    max_tokens_per_gpu=max_tokens_per_gpu,\n",
    "                                    verbose=2\n",
    "                                )\n",
    "\n",
    "osft_lower_bound, osft_expected, osft_upper_bound = my_osft_estimator.estimate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaefc58e",
   "metadata": {},
   "source": [
    "## OSFT Estimation with Liger Kernels\n",
    "`BasicEstimator` includes support for Liger Kernels. Liger Kernels aim to drastically\n",
    "speed up the time needed to fine-tune LLM models as well as reduce the memory footprint\n",
    "of the fine-tuning process.\n",
    "\n",
    "Empirically, the main memory optimization of Liger Kernels is to recalculate the activated outputs\n",
    "of the model rather than directly storing them on the GPU for future use. This can drastically\n",
    "improve the memory footprint when training use very large batch sizes. \n",
    "\n",
    "For the purposes of this estimator, enabling Liger Kernels will force `_calc_outputs` to always be 0.\n",
    "\n",
    "In Training Hub, OSFT uses Liger Kernels by default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "12a4e81a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  2.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimations for ibm-granite/granite-3.3-2b-instruct:\n",
      "\n",
      "\n",
      "Summary:\n",
      "The expected amount of memory needed to run this model is about 76.3 GB\n",
      "The lower and upper bounds are 69.4 - 90.2 GB\n",
      "If you have 2 GPUs, you will need about 38.2 GB, with bounds of 34.7 - 45.1 GB per GPU\n",
      "\n",
      "\n",
      "Component Breakdown:\n",
      "Each GPU will need 18.0 GB to store the model parameters\n",
      "Each GPU will need 9.4 GB to store the optimizer states\n",
      "Each GPU will need 4.7 GB to store the gradients\n",
      "Each GPU will need 2.5 GB to store the intermediate activations\n",
      "Since Liger Kernels are being used, no additional memory is needed to store the outputs\n",
      "Up to 10.4 GB can be expected as overhead\n",
      "\n",
      "Decision:\n",
      "The proposed training setup should work for your hardware.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "my_liger_estimator = OSFTEstimator(num_gpus=num_gpus,\n",
    "                                    gpu_memory=gpu_memory,\n",
    "                                    model_path=model_path,\n",
    "                                    max_tokens_per_gpu=max_tokens_per_gpu,\n",
    "                                    verbose=2,\n",
    "                                    use_liger=True\n",
    "                                )\n",
    "\n",
    "liger_lower_bound, liger_expected, liger_upper_bound = my_liger_estimator.estimate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48c82224",
   "metadata": {},
   "source": [
    "## Perform Estimation with the convenience function\n",
    "\n",
    "For higher level usage, rather than needing to directly instantiate an estimator object,\n",
    "we have provided a simple convenience function named `estimate`, in which you can\n",
    "provide the standard initialization arguments for your estimator as well as the\n",
    "type of training method you want to estimate for, and you can immediately obtain the estimation bounds.\n",
    "\n",
    "To specify the estimation type, you can pass in `\"sft\"` to the `training_method` argument to\n",
    "estimate for SFT, or `\"osft\"` to estimate for OSFT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f8ede8ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  2.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimations for ibm-granite/granite-3.3-2b-instruct:\n",
      "\n",
      "\n",
      "Summary:\n",
      "The expected amount of memory needed to run this model is about 55.8 GB\n",
      "The lower and upper bounds are 50.8 - 66.0 GB\n",
      "If you have 2 GPUs, you will need about 27.9 GB, with bounds of 25.4 - 33.0 GB per GPU\n",
      "\n",
      "\n",
      "Component Breakdown:\n",
      "Each GPU will need 4.7 GB to store the model parameters\n",
      "Each GPU will need 9.4 GB to store the optimizer states\n",
      "Each GPU will need 4.7 GB to store the gradients\n",
      "Each GPU will need 2.5 GB to store the intermediate activations\n",
      "Each GPU will need 4.0 GB to store the outputs\n",
      "Up to 7.6 GB can be expected as overhead\n",
      "\n",
      "Decision:\n",
      "The proposed training setup should work for your hardware.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "conv_sft_lower_bound, conv_sft_expected, conv_sft_upper_bound = estimate(\n",
    "                                                                    training_method=\"sft\",\n",
    "                                                                    num_gpus=num_gpus,\n",
    "                                                                    gpu_memory=gpu_memory,\n",
    "                                                                    model_path=model_path,\n",
    "                                                                    max_tokens_per_gpu=max_tokens_per_gpu,\n",
    "                                                                    verbose=2\n",
    "                                                                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d2e3f526",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  2.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimations for ibm-granite/granite-3.3-2b-instruct:\n",
      "\n",
      "\n",
      "Summary:\n",
      "The expected amount of memory needed to run this model is about 84.0 GB\n",
      "The lower and upper bounds are 76.4 - 99.3 GB\n",
      "If you have 2 GPUs, you will need about 42.0 GB, with bounds of 38.2 - 49.7 GB per GPU\n",
      "\n",
      "\n",
      "Component Breakdown:\n",
      "Each GPU will need 18.0 GB to store the model parameters\n",
      "Each GPU will need 9.4 GB to store the optimizer states\n",
      "Each GPU will need 4.7 GB to store the gradients\n",
      "Each GPU will need 2.5 GB to store the intermediate activations\n",
      "Each GPU will need 3.5 GB to store the outputs\n",
      "Up to 11.5 GB can be expected as overhead\n",
      "\n",
      "Decision:\n",
      "The proposed training setup will likely work for your hardware.\n",
      "\n",
      "Ideal amount of extra memory required (to reach the\n",
      "upper bound of memory requirements): 1.7 GB\n"
     ]
    }
   ],
   "source": [
    "conv_osft_lower_bound, conv_osft_expected, conv_osft_upper_bound = estimate(\n",
    "                                                                        training_method=\"osft\",\n",
    "                                                                        num_gpus=num_gpus,\n",
    "                                                                        gpu_memory=gpu_memory,\n",
    "                                                                        model_path=model_path,\n",
    "                                                                        max_tokens_per_gpu=max_tokens_per_gpu,\n",
    "                                                                        verbose=2\n",
    "                                                                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ab863891",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  2.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimations for ibm-granite/granite-3.3-2b-instruct:\n",
      "\n",
      "\n",
      "Summary:\n",
      "The expected amount of memory needed to run this model is about 76.3 GB\n",
      "The lower and upper bounds are 69.4 - 90.2 GB\n",
      "If you have 2 GPUs, you will need about 38.2 GB, with bounds of 34.7 - 45.1 GB per GPU\n",
      "\n",
      "\n",
      "Component Breakdown:\n",
      "Each GPU will need 18.0 GB to store the model parameters\n",
      "Each GPU will need 9.4 GB to store the optimizer states\n",
      "Each GPU will need 4.7 GB to store the gradients\n",
      "Each GPU will need 2.5 GB to store the intermediate activations\n",
      "Since Liger Kernels are being used, no additional memory is needed to store the outputs\n",
      "Up to 10.4 GB can be expected as overhead\n",
      "\n",
      "Decision:\n",
      "The proposed training setup should work for your hardware.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "conv_liger_lower_bound, conv_liger_expected, conv_liger_upper_bound = estimate(\n",
    "                                                                        training_method=\"osft\",\n",
    "                                                                        num_gpus=num_gpus,\n",
    "                                                                        gpu_memory=gpu_memory,\n",
    "                                                                        model_path=model_path,\n",
    "                                                                        max_tokens_per_gpu=max_tokens_per_gpu,\n",
    "                                                                        verbose=2,\n",
    "                                                                        use_liger=True\n",
    "                                                                    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "training_hub",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
